================================================================================
ML MODEL PIPELINE DOCUMENTATION
================================================================================

This directory contains two separate ML pipelines for classifying Disorders of
Consciousness (DoC) patients into binary categories:
  - Class 0: MCS+/MCS- (Minimally Conscious State)
  - Class 1: UWS (Unresponsive Wakefulness Syndrome)

Both pipelines use embeddings generated by the SleepFM pretrained model but at
different granularities.


================================================================================
PIPELINE 1: PER-SEQUENCE MODEL
================================================================================

FILES:
  - preprocessing_per_sequence.py
  - train_per_sequence.py
  - predict_per_sequence.py

DATA SOURCE:
  - HDF5 Location: doc_embeddings/{subject_id}.hdf5
  - Dataset: 'embedding_per_sequence'
  - Shape per subject: (n_sequences, 128)

WHAT IS A SEQUENCE?
  - A sequence is a 30-second window (6 epochs × 5 seconds each)
  - Each sequence is represented by a single 128-dimensional embedding
  - This embedding is the OUTPUT of the transformer after pooling across tokens

EXAMPLE:
  For subject with 41 epochs:
    - n_sequences = 41 // 6 = 6 complete sequences (5 epochs discarded)
    - Data shape: (6, 128)
    - Each row in the CSV represents one 30-second sequence

CSV STRUCTURE:
  patient_id | sequence_id | diagnostic_label | 0 | 1 | 2 | ... | 127
  HK295      | 0           | 4                | 0.42 | -0.31 | ... | 0.89
  HK295      | 1           | 4                | 0.38 | -0.27 | ... | 0.92
  ...

TRAINING APPROACH:
  - GroupKFold cross-validation with patient_id as groups
  - This ensures NO patient appears in both train and test sets
  - Prevents data leakage from multiple sequences of the same patient

PREDICTION AGGREGATION:
  Sequence-level → Subject-level via:
    - Majority voting (most common prediction)
    - Mean probability (average probability across sequences)

USE CASE:
  Best when you want:
    - Fewer samples per subject (less redundancy)
    - Embeddings that capture longer temporal context (30 seconds)
    - The "pooled" representation that the model was trained to produce


================================================================================
PIPELINE 2: PER-TOKEN MODEL
================================================================================

FILES:
  - preprocessing_per_token.py
  - train_per_token.py
  - predict_per_token.py

DATA SOURCE:
  - HDF5 Location: doc_embeddings_per_epoch/{subject_id}.hdf5
  - Dataset: 'sequence'
  - Shape per subject: (n_sequences, tokens_per_sequence, 128)
    where tokens_per_sequence = 6 (one token per 5-second epoch)

WHAT IS A TOKEN?
  - A token is a 5-second window (one epoch)
  - Each token is represented by a 128-dimensional embedding
  - This embedding is the transformer output BEFORE pooling
  - The token captures information at the finest granularity

EXAMPLE:
  For subject HK295 with 41 epochs:
    - n_sequences = 6, tokens_per_sequence = 6
    - Data shape: (6, 6, 128) → flattened to (36, 128)
    - Each row in the CSV represents one 5-second token

CSV STRUCTURE:
  patient_id | sequence_id | token_id | diagnostic_label | 0 | 1 | ... | 127
  HK295      | 0           | 0        | 4                | 0.42 | -0.31 | ...
  HK295      | 0           | 1        | 4                | 0.39 | -0.28 | ...
  HK295      | 0           | 2        | 4                | 0.41 | -0.30 | ...
  ...
  HK295      | 1           | 0        | 4                | 0.38 | -0.27 | ...
  ...

TRAINING APPROACH:
  - GroupKFold cross-validation with patient_id as groups
  - Same leakage prevention as per-sequence model
  - More training samples per subject (6× more than per-sequence)

PREDICTION AGGREGATION:
  Token-level → Sequence-level → Subject-level via:
    - Majority voting or mean probability at each level
    - Provides finer-grained analysis of temporal patterns

USE CASE:
  Best when you want:
    - Maximum number of training samples
    - Fine-grained temporal analysis (5-second resolution)
    - To study how predictions vary within sequences


================================================================================
DATA LEAKAGE PREVENTION
================================================================================

PROBLEM:
  Each patient has multiple samples (sequences or tokens). If we randomly split
  the data, the same patient could appear in both train and test sets. This is
  DATA LEAKAGE because:
    - Samples from the same patient are correlated
    - The model could learn patient-specific features instead of diagnosis features
    - Test set performance would be artificially inflated

SOLUTION: GroupKFold
  - Splits data so that all samples from a patient are in the same fold
  - Training and test sets have ZERO patient overlap
  - Provides realistic estimate of performance on new patients

IMPLEMENTATION:
  ```python
  from sklearn.model_selection import GroupKFold
  
  gkf = GroupKFold(n_splits=5)
  for train_idx, test_idx in gkf.split(X, y, groups=patient_ids):
      # All sequences/tokens from each patient stay together
      X_train, X_test = X[train_idx], X[test_idx]
      y_train, y_test = y[train_idx], y[test_idx]
  ```


================================================================================
COMPARISON TABLE
================================================================================

| Aspect              | Per-Sequence            | Per-Token               |
|---------------------|-------------------------|-------------------------|
| Granularity         | 30-second windows       | 5-second windows        |
| Samples per subject | n_epochs // 6           | (n_epochs // 6) × 6     |
| Embedding source    | After pooling           | Before pooling          |
| Temporal context    | Captures 30s context    | 5s snapshot             |
| Training samples    | Fewer, more independent | More, within-seq correl |
| Aggregation levels  | Sequence → Subject      | Token → Seq → Subject   |

EXAMPLE for subject with 42 epochs:
  - Per-sequence: 42 // 6 = 7 samples
  - Per-token: 7 × 6 = 42 samples


================================================================================
RUNNING THE PIPELINES
================================================================================

# Per-Sequence Pipeline
python preprocessing_per_sequence.py --main_path /path/to/doc_embeddings
python train_per_sequence.py --data_path ./data_per_sequence/embeddings_per_sequence.csv
python predict_per_sequence.py --data_path ./data_per_sequence/embeddings_per_sequence.csv --output_path predictions.csv

# Per-Token Pipeline
python preprocessing_per_token.py --main_path /path/to/doc_embeddings_per_epoch
python train_per_token.py --data_path ./data_per_token/embeddings_per_token.csv
python predict_per_token.py --data_path ./data_per_token/embeddings_per_token.csv --output_path predictions.csv


================================================================================
OUTPUT FILES
================================================================================

Per-Sequence Model (trained_model_per_sequence/):
  - random_forest_per_sequence.joblib    # Trained model
  - best_params_per_sequence.txt         # Best hyperparameters
  - metrics_per_sequence.txt             # Test set metrics
  - evaluation_plots_per_sequence.png    # ROC, PR curves, confusion matrix
  - grid_search_results_per_sequence.csv # All CV results
  - test_patient_ids.txt                 # Patients in test set

Per-Token Model (trained_model_per_token/):
  - random_forest_per_token.joblib
  - best_params_per_token.txt
  - metrics_per_token.txt
  - evaluation_plots_per_token.png
  - grid_search_results_per_token.csv
  - test_patient_ids.txt

================================================================================
